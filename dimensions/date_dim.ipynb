{"cells":[{"cell_type":"code","source":["# Creation of schema by name \"dimension\" that hold all the dimensions (rider, date, time, station) together\nspark.sql(\"CREATE SCHEMA IF NOT EXISTS dimension\")\n\n# Drop if the rider dimension exists\nspark.sql(\"DROP TABLE IF EXISTS dimension.date\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d5cb914e-031a-4ea5-a55e-ef711c563a93","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[1]: DataFrame[]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[1]: DataFrame[]"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.types import StringType\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import DateType\n\n# Read the trips and payments staging table\ntrips_df = spark.table(\"staging.trips\")\npayments_df = spark.table(\"staging.payments\")\n\n# Extract the min date from the payments & max date (with an offset of 5 years) from the trips\n# It will help you identify the date range to buil the date dimension\nmin_dt = payments_df.select(to_date(min(\"date\")).alias(\"min_date\")).collect()[0][0]\nmax_dt = trips_df.select(add_months(to_date(max(\"ended_at\")),(24*5))).collect()[0][0]\n\n# Generate a sequence of data from min date to max date\n(\n  spark.sql(f\"select explode(sequence(to_date('{min_dt}'), to_date('{max_dt}'), interval 1 day)) as calendarDate\")\n    .createOrReplaceTempView('dates_dim_temp')\n)\n\n# transform SQL statement to prepare the date dimension\nfinal_dim = \\\n    spark.sql(\"\\\n          Select \\\n              calendarDate as date \\\n              ,year(calendarDate) as year \\\n              ,month(calendarDate) as month \\\n              ,quarter(calendarDate) as quarter \\\n              ,day(CalendarDate) as day \\\n              ,weekofyear(CalendarDate) as week \\\n              ,dayofweek(CalendarDate) as dayofweek \\\n          From dates_dim_temp\")\n\nprint(final_dim.show())\n\n# Saves the data as a table in delta location.\nfinal_dim.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dimension.date\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78215a2e-11c6-41e6-8ead-5dbdbd9b6e58","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+----+-----+-------+---+----+---------+\n|      date|year|month|quarter|day|week|dayofweek|\n+----------+----+-----+-------+---+----+---------+\n|2013-02-01|2013|    2|      1|  1|   5|        6|\n|2013-02-02|2013|    2|      1|  2|   5|        7|\n|2013-02-03|2013|    2|      1|  3|   5|        1|\n|2013-02-04|2013|    2|      1|  4|   6|        2|\n|2013-02-05|2013|    2|      1|  5|   6|        3|\n|2013-02-06|2013|    2|      1|  6|   6|        4|\n|2013-02-07|2013|    2|      1|  7|   6|        5|\n|2013-02-08|2013|    2|      1|  8|   6|        6|\n|2013-02-09|2013|    2|      1|  9|   6|        7|\n|2013-02-10|2013|    2|      1| 10|   6|        1|\n|2013-02-11|2013|    2|      1| 11|   7|        2|\n|2013-02-12|2013|    2|      1| 12|   7|        3|\n|2013-02-13|2013|    2|      1| 13|   7|        4|\n|2013-02-14|2013|    2|      1| 14|   7|        5|\n|2013-02-15|2013|    2|      1| 15|   7|        6|\n|2013-02-16|2013|    2|      1| 16|   7|        7|\n|2013-02-17|2013|    2|      1| 17|   7|        1|\n|2013-02-18|2013|    2|      1| 18|   8|        2|\n|2013-02-19|2013|    2|      1| 19|   8|        3|\n|2013-02-20|2013|    2|      1| 20|   8|        4|\n+----------+----+-----+-------+---+----+---------+\nonly showing top 20 rows\n\nNone\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+----+-----+-------+---+----+---------+\n|      date|year|month|quarter|day|week|dayofweek|\n+----------+----+-----+-------+---+----+---------+\n|2013-02-01|2013|    2|      1|  1|   5|        6|\n|2013-02-02|2013|    2|      1|  2|   5|        7|\n|2013-02-03|2013|    2|      1|  3|   5|        1|\n|2013-02-04|2013|    2|      1|  4|   6|        2|\n|2013-02-05|2013|    2|      1|  5|   6|        3|\n|2013-02-06|2013|    2|      1|  6|   6|        4|\n|2013-02-07|2013|    2|      1|  7|   6|        5|\n|2013-02-08|2013|    2|      1|  8|   6|        6|\n|2013-02-09|2013|    2|      1|  9|   6|        7|\n|2013-02-10|2013|    2|      1| 10|   6|        1|\n|2013-02-11|2013|    2|      1| 11|   7|        2|\n|2013-02-12|2013|    2|      1| 12|   7|        3|\n|2013-02-13|2013|    2|      1| 13|   7|        4|\n|2013-02-14|2013|    2|      1| 14|   7|        5|\n|2013-02-15|2013|    2|      1| 15|   7|        6|\n|2013-02-16|2013|    2|      1| 16|   7|        7|\n|2013-02-17|2013|    2|      1| 17|   7|        1|\n|2013-02-18|2013|    2|      1| 18|   8|        2|\n|2013-02-19|2013|    2|      1| 19|   8|        3|\n|2013-02-20|2013|    2|      1| 20|   8|        4|\n+----------+----+-----+-------+---+----+---------+\nonly showing top 20 rows\n\nNone\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"date_dim","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1845019978588551}},"nbformat":4,"nbformat_minor":0}
